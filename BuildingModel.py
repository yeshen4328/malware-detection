import Graph
import tensorflow as tf
import pickle
import copy
from Graph import traverse
import os
import numpy as np
from multiprocessing import Process
import gc
from LocalClass import APK
from gensim.models.doc2vec import Doc2Vec

batch_size = 30
feature_nums = 20
state_size = 20
max_seq_len = 100
num_layers = 5
methods2codes = pickle.load(open('methods_codes_dict.pkl','rb'))
api2vec = Doc2Vec.load('apimodel')
import numpy as np
def taskPerCore(apks, set_dir_path, dataset, packages):
    for i, apk in enumerate(apks):
        if(os.path.isfile('../apk_graph_labeled_test/' + apk + '.pkl')):
            print('jump ' + apk)
            continue
        print('processing: '+apk)
        apk_path = set_dir_path + '/' + apk
        if (dataset == 'benign'):
            label = 0
        else:
            label = 1
        print(label)
        # sum_path include a lot of paths, element of whchi is the index of methods
        sum_path, index2methods = traverse(apk_path, label, packages)
        apk_struct = (sum_path, label, index2methods)
        pickle.dump(apk_struct, open('../apk_graph_labeled_test/' + apk + '.pkl', 'wb'))
        apk_struct = None
        sum_path = None
        index2methods = None
        if(i % 10 == 0):
            gc.collect()
def loadPakcages():
    p=[]
    with open("PackagesOcc.txt", 'r') as lines:
        for line in lines:
            p.append(line.replace('\n', ''))
    return p
def gen_apk_struct():
    path = "../filterGraph"
    datasets = os.listdir(path)
    setNum = len(datasets)
    for dataset in datasets:
        set_dir_path = path + '/' + dataset
        apks = os.listdir(set_dir_path)
        core = 1
        sampleNum = len(apks)
        samplesPerCore = sampleNum // core
        processlist = []
        apksPerCore = []
        for i in range(core):
            if (sampleNum - samplesPerCore * i >= samplesPerCore):
                apksPerCore = apks[i * samplesPerCore: (i + 1) * samplesPerCore]
            else:
                apksPerCore = apks[i * samplesPerCore: sampleNum]
            processlist.append(Process(target=taskPerCore, args=(apksPerCore, set_dir_path, dataset, packages)))
            processlist[i].daemon = True
            processlist[i].start()
        for i in range(core):
            processlist[i].join()
def gen_batchdata(root, vec_path ):
    #root = '../apk_graph_labeld/'
    path_list = os.listdir(root)
    total_app_num = len(path_list)
    batch_num = total_app_num // batch_size
    for batch_i in range(batch_num):
        batch_list = []
        label_list = []
        for i in range(min(batch_size, len(path_list))):
            print('batch: ' + str(batch_i))
            rand_i = np.random.randint(0, len(path_list))
            path = path_list[rand_i]
            path_list.remove(path)
            if(os.path.isfile(vec_path+path)):
                print('loading vec data....')
                struct = pickle.load(open(vec_path+path,'rb'))
                graph = struct[0]
                label = struct[1]
            else:
                print('load raw data....')
                struct = pickle.load(open(root + path,'rb'))
                graph, label = getvec_and_padding(struct)
            if (not os.path.isfile(vec_path + path)):
                apk = APK(graph, label)
                pickle.dump(apk, open(vec_path + path, 'wb'))

            batch_list.append(graph)
            label_list.append(label)

        print('batch_list_size' + str(len(batch_list)))
        yield batch_list, label_list
def gen_epoch():
    num_epoch = 5
    for i in range(num_epoch):
        print('epoch: '+ str(i))
        yield gen_batchdata(root = '../apk_graph_labeld/',vec_path = '../apk_graph_vec/')

def getvec_and_padding(struct):
    graph = struct[0]
    label = struct[1]
    index2methods = struct[2]
    print('transform to vec...')
    for i, list in enumerate(graph):
        for j, item in enumerate(list):
            api = index2methods[item]
            vec = getMethodVec(api)
            list[j] = vec
        while(len(list) < max_seq_len):
            list.append([0 for p in range(feature_nums)])
        graph[i] = list
    return graph, label

def getMethodVec(apicall):
    if(isFramwrokApis(apicall)):
        classes = apicall.split(': ')[0]
        method = apicall.split(': ')[1]
        classes = classes.split('.')
        # return_type = method.split(' ')[0].split('.')
        method_name = method.split(' ')[1].split('(')[0]
        sentence = classes + [method_name]
    else:
        sentence = ['selfdefined']
    vec = api2vec.infer_vector(sentence)
    return vec
packages = loadPakcages()
def isFramwrokApis(api):
    for line in packages:
        if api.startswith(line):
            return True

with tf.variable_scope('rnn_cell'):
    U = tf.get_variable('U', [feature_nums, state_size])
    W = tf.get_variable('W', [state_size, state_size])
def rnn_cell(in_state, input_x):
    with tf.variable_scope('rnn_cell', reuse=True):
        U = tf.get_variable('U', [feature_nums, state_size])
        W = tf.get_variable('W', [state_size, state_size])
    return tf.tanh(tf.matmul(input_x, U) + tf.matmul(in_state, W))
def model():
    print('buiding tf model')
    #>>>>>>>>>>>>>>>>>>>>>RNN model<<<<<<<<<<<<<<<<<<<<<<<<<<
    # data = tf.placeholder(dtype = tf.float32, shape=[None, max_seq_len, feature_nums], name='batch_data')
    # input_lable = tf.placeholder(dtype = tf.float32, shape=[1], name='lable')
    # app_size = tf.shape(data)[0]
    # state = tf.zeros(shape=[app_size, state_size], dtype=tf.float32, name='init_state')
    # unstack_data = tf.unstack(data, axis=1)
    # for step in unstack_data:
    #     state = rnn_cell(state, step)
    # final_state_mean = tf.reduce_mean(state, 0, keep_dims=True)

    # data = tf.placeholder(dtype=tf.float32, shape=[batch_size, None, max_seq_len, feature_nums], name='batch_data')
    # input_lable = tf.placeholder(dtype=tf.float32, shape=[batch_size, 1], name='lable')
    # final_state_mean_sum = []
    # for i in range(batch_size):
    #     _app = data[i,:,:,:]
    #     unstack_app = tf.unstack(_app, axis=1)
    #     app_size = tf.shape(unstack_app)[1]
    #     state = tf.zeros(shape=[app_size, state_size], dtype=tf.float32, name='init_state')
    #     for step in unstack_app:
    #         state = rnn_cell(state, step)
    #     final_state_mean = tf.reduce_mean(state, 0)
    #     final_state_mean_sum.append(final_state_mean)
    #>>>>>>>>>>>>>>>>>>>  LSTM model  <<<<<<<<<<<<<<<<<<<<<<
    data = tf.placeholder(dtype=tf.float32, shape=[batch_size, None, max_seq_len, feature_nums], name = 'batch_data')
    input_lable = tf.placeholder(dtype=tf.float32, shape=[batch_size, 1], name = 'lable')
    _real_app_size = tf.placeholder(dtype=tf.int32, shape=[batch_size], name = 'real_app_size')
    final_state_mean_sum = []
    lstm = tf.contrib.rnn.BasicLSTMCell(state_size)
    dropout_cell = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=0.8)
    muti_lstm = tf.nn.rnn_cell.MultiRNNCell([dropout_cell] * num_layers, state_is_tuple=True)
    for i in range(batch_size):
        _app = data[i, :, :, :]
        unstack_app = tf.unstack(_app, axis=1)
        app_size = tf.shape(unstack_app)[1]  #the number of paths in one app
        hidden_state = tf.zeros(shape=[app_size, state_size], dtype=tf.float32, name='hidden_state')
        current_state = tf.zeros(shape=[app_size, state_size], dtype=tf.float32, name='hidden_state')
        state = hidden_state, current_state
        for step in unstack_app:
            output, state = lstm(step, state)
        final_current_state = output
        final_state_mean = tf.reduce_mean(final_current_state[0:_real_app_size[i], :], 0)# >>>>>>>>>need change<<<<<<<<<<
        final_state_mean_sum.append(final_state_mean)


# >>>>>>>>>>>>>>>>>>>>>>>svm_model(se_sum, input_lable)<<<<<<<<<<<<<<<<<<<<<<<<
    print('building svm model')
    svmC = tf.constant([0.01])
    # Define and initialize the network.
    # w = tf.get_variable('w', [state_size, 1])
    # b = tf.get_variable('b', [1])
    # y_raw = tf.matmul(final_state_mean, w) + b

    w = tf.get_variable('w', [state_size, 1])
    b = tf.get_variable('b', [batch_size, 1])
    y_raw = tf.matmul(final_state_mean_sum, w) + b
    prediction =  tf.sign(tf.nn.relu(y_raw))

    print('optimization')
    # Optimization.
    regularization_loss = tf.constant([0.1]) * tf.reduce_sum(tf.square(w))
    # hinge_loss = tf.reduce_sum(tf.maximum(tf.zeros([1]), 1 - input_lable * y_raw));
    hinge_loss = tf.reduce_mean(tf.maximum(0., 1 - input_lable * y_raw))
    svm_loss = regularization_loss + hinge_loss
    train_step = tf.train.GradientDescentOptimizer(0.005).minimize(svm_loss)

    accu = 1 - tf.reduce_mean(tf.cast(tf.abs(prediction - input_lable), tf.float32))
# >>>>>>>>>>>>>>>>>>>>>>>>>>>train<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
    print('train start')
    with tf.Session() as sess:
        init_op = tf.global_variables_initializer()
        sess.run(init_op)
        it = 0
        accu_all = 0.
        for epoch in gen_epoch():
            for batch_data, labels in epoch:
                print(str(len(labels))+ ' ' + str(np.shape(labels)))
                padded_batch_data, real_app_size = padding(batch_data)
                print('label: ' + str(labels))
                loss,_ = sess.run([svm_loss, train_step], feed_dict={data: padded_batch_data, input_lable: [[l] for l in labels], _real_app_size:real_app_size})
                print('loss: ' + str(loss))
            print('testing...')
            test_data,test_label = gentestdata()
            p_test_data, p_real_app_size = padding(test_data)
            _accu1 = sess.run([accu], feed_dict={data : p_test_data[0:batch_size],
                                                input_lable : [[l] for l in test_label[0:batch_size]],
                                                _real_app_size : p_real_app_size[0:batch_size]})
            test_data, test_label = gentestdata()
            p_test_data, p_real_app_size = padding(test_data)
            _accu2 = sess.run([accu], feed_dict={data: p_test_data[0:batch_size],
                                                input_lable: [[l] for l in test_label[0:batch_size]],
                                                _real_app_size: p_real_app_size[0:batch_size]})
            _accu = np.divide(np.add(_accu1,_accu2),2)
            print('accuracy : ' + str(_accu))
    # with tf.Session() as sess:
    #     init_op = tf.global_variables_initializer()
    #     sess.run(init_op)
    #     for epoch in gen_epoch():
    #         for batch_data, labels in epoch:
    #             print(np.shape(batch_data))
    #             for ind, app in enumerate(batch_data):
    #                 loss,_ = sess.run([svm_loss, train_step], feed_dict={data: app, input_lable: [labels[ind]]})
    #                 print('loss: ' + str(loss))

def gentestdata():
    batch_list = []
    label_list = []
    root = '../apk_graph_labeled_test/'
    vec_path = '../apk_graph_vec_test/'
    path_list = os.listdir(root)
    for i in range(batch_size):
        rand_i = np.random.randint(0, len(path_list))
        path = path_list[rand_i]
        path_list.remove(path)
        if (os.path.isfile(vec_path + path)):
            print('loading vec data....')
            struct = pickle.load(open(vec_path + path, 'rb'))
            graph = struct[0]
            label = struct[1]
        else:
            print('load raw data....')
            struct = pickle.load(open(root + path, 'rb'))
            graph, label = getvec_and_padding(struct)
        batch_list.append(graph)
        label_list.append(label)
        if (not os.path.isfile(vec_path + path)):
            apk = APK(graph, label)
            pickle.dump(apk, open(vec_path + path, 'wb'))
    print('batch_list_size' + str(len(batch_list)))
    return batch_list,label_list

def padding(batch_data):
    max_app_len = 0
    max_path_len = 0
    real_app_size = []
    for app in batch_data:
        max_app_len = max(max_app_len, len(app))
        real_app_size.append(len(app))
    batch_array = np.zeros(shape=(batch_size, max_app_len, max_seq_len, feature_nums))
    print(np.shape(batch_array))
    for i, app in enumerate(batch_data):
        print(np.shape(app))
        tmp_len = len(app)
        batch_array[i][0:tmp_len] = app
    return batch_array, real_app_size
if __name__ == "__main__":
    gen_apk_struct()
    #model()
    #traverse()

